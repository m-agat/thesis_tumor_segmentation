TODO:
- split the overlay for each tissue
- overall uncertainty and uncertainty per tissue type 

- create mock missing modalities for new data
- type of the tumor will affect the prediction 

DONE:
- convert the segmentations to nifti files 
- get matlab 
- uncertainty: how much the predictions differ from the ground truth 
- we need to know whether one model is better for one tissue type than another (and modalities)
- spm12, cut12
- weight the different models when computing uncertainty
- run the preprocessing again but correctly

Notes & Issues:

# Ensemble prediction
The ensemble of models is now using majority voting to choose the label for the voxel. 
In the majority voting a mode is utilized.
This is because the outputs of the segmentation are hard labels not probabilities.

# Weighted ensemble 
The weighted ensemble uses two types of weights:
1. dice score per model and per tissue
2. class size: to reduce the impact of the larger classes e.g. background

The weighted ensemble also uses majority voting but in a slightly different way.
Instead of taking the mode like in the normal ensemble, we take the class
with the highest total vote (np.argmax). This is because we now accumu;ate the WEIGHTED VOTES 
So, after computing the total weighted votes for each class at each voxel, the class with the highest total vote.

# Preprocessing new data 
Some images are pixalated in saggital and coronal planes (e.g. MMPG)

I tried to run predictions using an nnUNet on the new data, but some patients have fewer than 4 modalities 
(e.g. only FLAIR and T1GAD) what made the inference impossible to make. 

Show the nnUNet result on patient ARE

- !!! uncertainties on the voxel level
- generate images of one slice for all patients
- retrain the models on more epochs
- combine the models (with uncertainties and without): compare the two approaches 

- try histogram filters
- ask Alfredo about the performance metrics


Normalization:
- segment a tumor (mean of all of the algorithms)
- create a mask
- normalize the image with the mask
- take a map of the verticale
- take the ventricals to the native space



- location of the tumor 
- probability of being csf - a map of regions that have smaller of probability of being part of the tumor 


# prob_maps = {1: [], 2: [], 4: []}

        # # Generate weighted binary maps for voting
        # for model_idx, model_pred in enumerate(model_predictions):
        #     for tissue_idx, tissue_label in enumerate([1, 2, 4]):  
        #         binary_map = (model_pred[:, tissue_idx] > 0.5).float()  
        #         weight_expanded = weights[:, model_idx, tissue_idx].view(-1, 1, 1, 1)  
        #         weighted_vote = binary_map * weight_expanded
        #         prob_maps[tissue_label].append(weighted_vote.detach().cpu().numpy()) 

        # # Aggregate votes across models and assign final class based on max votes
        # ensemble_seg = torch.zeros((1, 96, 96, 96), dtype=torch.int8).to(config.device)
        # for tissue_label, weighted_votes in prob_maps.items():
        #     total_votes = np.sum(weighted_votes, axis=0)
        #     tissue_mask = (total_votes > 0.5).astype(np.int8)  
        #     ensemble_seg[tissue_mask == 1] = tissue_label